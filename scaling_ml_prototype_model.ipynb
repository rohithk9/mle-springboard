{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8e2ce1",
   "metadata": {},
   "source": [
    "## For scaling the prototyped model in the previous step the following options can be considered\n",
    "\n",
    "#### Please note that my dataset is not large and this is only an example of how I would scale the prototype in case I was dealing with bigger dataset like real world applications\n",
    "\n",
    "#### 1. Batch Processing for Large Datasets (Using Dask)\n",
    "Dask allows you to process larger-than-memory datasets by breaking them into manageable chunks. If the dataset is too large to fit in memory, we can use Dask to handle chunks of the data in parallel and perform operations like TF-IDF vectorization in batches.\n",
    "\n",
    "#### 2. Sparse Matrices and Dimensionality Reduction\n",
    "This option is useful when you're dealing with very high-dimensional data (e.g., after TF-IDF vectorization). Using sparse matrices helps reduce memory usage, and dimensionality reduction techniques like Truncated SVD can speed up the cosine similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d2bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import os\n",
    "import pickle\n",
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef26117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scopes for the Google Drive API\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "def authenticate_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('./token.pickle'):\n",
    "        with open('./token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'new_client_id.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "drive_service = authenticate_google_drive()\n",
    "\n",
    "def download_file(file_id, file_name):\n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    fh = io.FileIO(file_name, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(f\"Download {int(status.progress() * 100)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e48024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download 100%.\n"
     ]
    }
   ],
   "source": [
    "# All the datasets can be found here in this drive folder \n",
    "# https://drive.google.com/drive/folders/14J0u4AhUwkfKJKYnJzuD6dRW2y9uZqnD?usp=sharing\n",
    "# dataset from beer advocate\n",
    "# downloaded from https://www.kaggle.com/datasets/thedevastator/1-5-million-beer-reviews-from-beer-advocate\n",
    "file_id = '1CmgbvYGtgp0b7wZU8z8FT8tbRCJ6WkYT'\n",
    "file_name = 'beer_and_profile_ratings.csv'\n",
    "download_file(file_id, file_name)\n",
    "\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31af150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3197 entries, 0 to 3196\n",
      "Data columns (total 25 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Name               3197 non-null   object \n",
      " 1   Style              3197 non-null   object \n",
      " 2   Brewery            3197 non-null   object \n",
      " 3   Beer Name (Full)   3197 non-null   object \n",
      " 4   Description        3197 non-null   object \n",
      " 5   ABV                3197 non-null   float64\n",
      " 6   Min IBU            3197 non-null   int64  \n",
      " 7   Max IBU            3197 non-null   int64  \n",
      " 8   Astringency        3197 non-null   int64  \n",
      " 9   Body               3197 non-null   int64  \n",
      " 10  Alcohol            3197 non-null   int64  \n",
      " 11  Bitter             3197 non-null   int64  \n",
      " 12  Sweet              3197 non-null   int64  \n",
      " 13  Sour               3197 non-null   int64  \n",
      " 14  Salty              3197 non-null   int64  \n",
      " 15  Fruits             3197 non-null   int64  \n",
      " 16  Hoppy              3197 non-null   int64  \n",
      " 17  Spices             3197 non-null   int64  \n",
      " 18  Malty              3197 non-null   int64  \n",
      " 19  review_aroma       3197 non-null   float64\n",
      " 20  review_appearance  3197 non-null   float64\n",
      " 21  review_palate      3197 non-null   float64\n",
      " 22  review_taste       3197 non-null   float64\n",
      " 23  review_overall     3197 non-null   float64\n",
      " 24  number_of_reviews  3197 non-null   int64  \n",
      "dtypes: float64(6), int64(14), object(5)\n",
      "memory usage: 624.5+ KB\n",
      "None\n",
      "(3197, 25)\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ecc623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                 0\n",
       "Style                0\n",
       "Brewery              0\n",
       "Beer Name (Full)     0\n",
       "Description          0\n",
       "ABV                  0\n",
       "Min IBU              0\n",
       "Max IBU              0\n",
       "Astringency          0\n",
       "Body                 0\n",
       "Alcohol              0\n",
       "Bitter               0\n",
       "Sweet                0\n",
       "Sour                 0\n",
       "Salty                0\n",
       "Fruits               0\n",
       "Hoppy                0\n",
       "Spices               0\n",
       "Malty                0\n",
       "review_aroma         0\n",
       "review_appearance    0\n",
       "review_palate        0\n",
       "review_taste         0\n",
       "review_overall       0\n",
       "number_of_reviews    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4af67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Style', 'Brewery', 'Beer Name (Full)', 'Description', 'ABV',\n",
       "       'Min IBU', 'Max IBU', 'Astringency', 'Body', 'Alcohol', 'Bitter',\n",
       "       'Sweet', 'Sour', 'Salty', 'Fruits', 'Hoppy', 'Spices', 'Malty',\n",
       "       'review_aroma', 'review_appearance', 'review_palate', 'review_taste',\n",
       "       'review_overall', 'number_of_reviews'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc52a7",
   "metadata": {},
   "source": [
    "## Calculating feature wise similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22416fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "beer_style_vectors = vectorizer.fit_transform(df['Style'])\n",
    "body_vectors = df['Body'].to_numpy().reshape(-1, 1)\n",
    "beer_desc_vectors = vectorizer.fit_transform(df['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c1bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarities for these vectors\n",
    "style_similarity = cosine_similarity(beer_style_vectors)\n",
    "body_similarity = cosine_similarity(body_vectors)\n",
    "\n",
    "# calculating combined similarity of style and body similarities\n",
    "combined_similarity = (style_similarity + body_similarity)/2\n",
    "combined_similarity_df = pd.DataFrame(combined_similarity, index=df.index, columns=df.index)\n",
    "\n",
    "# calculating description similarities\n",
    "desc_similarity = cosine_similarity(beer_desc_vectors)\n",
    "\n",
    "desc_similarity_df = pd.DataFrame(desc_similarity, index=df.index, columns=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525be99f",
   "metadata": {},
   "source": [
    "## Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556bcae6",
   "metadata": {},
   "source": [
    "### Our problem is a cold start problem\n",
    "#### In our implementation, we will be taking inputs from users on what kind of or style of beer they would like to try and ask if they would like a certain minimum level of abv in their beer\n",
    "#### This means that we would need to use features based on the content of the user input and compare it to what we have in our dataset and return the closest matching recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfae334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based_recs(style, abv=None, refresh=False):\n",
    "    global start_idx\n",
    "    #get similar styles of beers\n",
    "    target_indices = df.index[df['Style'].str.contains(style, case=False, na=False)].tolist()\n",
    "    num_recs = 5\n",
    "    # If refresh is True, show next set of recommendations\n",
    "    if refresh:\n",
    "        start_idx += num_recs\n",
    "    else:\n",
    "        start_idx = 0  # Reset the index when not refreshing\n",
    "\n",
    "    # Step 3: Get similar beers based on the description similarity\n",
    "    for idx in target_indices:\n",
    "        similar_beers = combined_similarity_df[idx].sort_values(ascending=False)\n",
    "        similar_beers_by_desc = desc_similarity_df[idx].sort_values(ascending=False)\n",
    "        \n",
    "        top_recs = similar_beers.head(20)\n",
    "        top_desc_recs = similar_beers_by_desc.head(20)\n",
    "        \n",
    "        beer_recommendations = df.loc[top_recs.index, ['Style', 'Name', 'ABV']]\n",
    "        beer_recommendations['similarity_score'] = top_recs.values\n",
    "        \n",
    "        desc_recommendations = df.loc[top_desc_recs.index, ['Style', 'Name', 'ABV']]\n",
    "        desc_recommendations['similarity_score'] = top_desc_recs.values\n",
    "        \n",
    "        if (abv is not None):\n",
    "            if not isinstance(abv, (int, float)):\n",
    "                print(f\"{abv} is not a valid number\")\n",
    "                abv=0\n",
    "                beer_recommendation = beer_recommendations.loc[df['ABV'] >= abv]\n",
    "                desc_recommendations = desc_recommendations.loc[df['ABV'] >= abv]\n",
    "            else:\n",
    "                beer_recommendation = beer_recommendations.loc[df['ABV'] >= abv]\n",
    "                desc_recommendations = desc_recommendations.loc[df['ABV'] >= abv]\n",
    "    \n",
    "    # Slice the DataFrame based on the start index and number of recommendations to show\n",
    "    current_beer_recs = beer_recommendations.iloc[start_idx:start_idx + num_recs, :]\n",
    "    current_desc_recs = desc_recommendations.iloc[start_idx:start_idx + num_recs, :]\n",
    "        \n",
    "    print(\"Here are the recommended beers for you! \\n Cheers!! \\n\")\n",
    "    # Print DataFrame in tabular format using 'tabulate'\n",
    "    print(tabulate(current_beer_recs.iloc[:5,:]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c7a44",
   "metadata": {},
   "source": [
    "#### As the name suggests, this method returns the most popular beers based on number of reviews and overall review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0be844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_based_recs(num_recs=5):\n",
    "    popular_beers = df[['Name', 'review_overall', 'number_of_reviews']].sort_values(\n",
    "        by=['review_overall', 'number_of_reviews'], ascending=False).head(num_recs)\n",
    "    \n",
    "    print(f\"Top {num_recs} popular beers:\")\n",
    "    print(tabulate(popular_beers, headers=\"keys\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501432c",
   "metadata": {},
   "source": [
    "#### This filters the beers based on the feature defined and returns the top beers for that beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c7fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_beers_by_feature(feature, num_recs=5):\n",
    "    top_beers = df[['Name', feature]].sort_values(by=feature, ascending=False).head(num_recs)\n",
    "    \n",
    "    print(f\"Top {num_recs} beers by {feature}:\")\n",
    "    print(tabulate(top_beers, headers=\"keys\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1ce6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Based Recommendations:\n",
      "Here are the recommended beers for you! \n",
      " Cheers!! \n",
      "\n",
      "----  --------------  ---------------------------------------  ----  -\n",
      "1559  Lager - Vienna  Saranac Season's Best - Nut Brown Lager  5.3   1\n",
      "1554  Lager - Vienna  Penn Pilsner                             5     1\n",
      "1563  Lager - Vienna  Jenny Lake Lager                         4.8   1\n",
      "1562  Lager - Vienna  The Raven Special Lager                  5.25  1\n",
      "1561  Lager - Vienna  Rusty Chain                              5.2   1\n",
      "----  --------------  ---------------------------------------  ----  -\n",
      "\n",
      "Popularity-Based Recommendations:\n",
      "Top 5 popular beers:\n",
      "      Name                             review_overall    number_of_reviews\n",
      "----  -----------------------------  ----------------  -------------------\n",
      "1675  Lambik (2 Year Old Unblended)           5                          4\n",
      "3138  Sang Noir                               4.80769                   13\n",
      " 764  Helios Goya Dry                         4.75                       2\n",
      "1653  De Troch Oude Gueuze                    4.75                       2\n",
      "1609  Framboos                                4.7                       10\n",
      "\n",
      "Top Beers by ABV:\n",
      "Top 5 beers by ABV:\n",
      "      Name                                                      ABV\n",
      "----  ------------------------------------------------------  -----\n",
      " 299  Schorschbock 57%                                        57.5\n",
      " 297  Schorschbock 31%                                        30.86\n",
      "2746  Samuel Adams UtopiasBoston Beer Company (Samuel Adams)  28\n",
      "2772  Colossus                                                20\n",
      "2513  Chocolate Rain                                          19.6\n"
     ]
    }
   ],
   "source": [
    "# Run all recommendation types\n",
    "style = 'lager'\n",
    "abv = 3\n",
    "\n",
    "print(\"Content Based Recommendations:\")\n",
    "content_based_recs(style, abv)\n",
    "\n",
    "print(\"\\nPopularity-Based Recommendations:\")\n",
    "popularity_based_recs()\n",
    "\n",
    "print(\"\\nTop Beers by ABV:\")\n",
    "top_beers_by_feature('ABV')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654058e",
   "metadata": {},
   "source": [
    " ## Scaling using batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c394aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: dask in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (2023.6.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from scipy) (1.24.3)\n",
      "Requirement already satisfied: click>=8.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (8.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (23.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from dask) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=4.13.0->dask) (3.11.0)\n",
      "Requirement already satisfied: locket in /Users/rohitkulkarni/anaconda3/lib/python3.11/site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef5efdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a654c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<truediv, shape=(3197, 3197), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Load large dataset using Dask\n",
    "df = dd.read_csv(file_name)\n",
    "\n",
    "# Process the 'Style' and 'Description' columns in batches using Dask\n",
    "def compute_tfidf_batch(df_chunk, column_name):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df_chunk[column_name])\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Process the 'Style' and 'Description' columns\n",
    "style_tfidf = df.map_partitions(lambda df_chunk: compute_tfidf_batch(df_chunk, 'Style')).compute()\n",
    "description_tfidf = df.map_partitions(lambda df_chunk: compute_tfidf_batch(df_chunk, 'Description')).compute()\n",
    "\n",
    "# Convert to sparse matrix\n",
    "style_tfidf_sparse = csr_matrix(style_tfidf)\n",
    "desc_tfidf_sparse = csr_matrix(description_tfidf)\n",
    "\n",
    "# Normalize the vectors\n",
    "style_tfidf_sparse = normalize(style_tfidf_sparse)\n",
    "desc_tfidf_sparse = normalize(desc_tfidf_sparse)\n",
    "\n",
    "# Define cosine similarity manually\n",
    "def cosine_similarity_manual(X, Y):\n",
    "    return 1 - cosine(X, Y)\n",
    "\n",
    "# Now, instead of using dask cosine_similarity, we manually compute similarity\n",
    "def compute_cosine_similarity_sparse(X_sparse, Y_sparse):\n",
    "    # Convert sparse matrix to Dask Array\n",
    "    X_dask = da.from_array(X_sparse.toarray(), chunks=(1000, 1000))\n",
    "    Y_dask = da.from_array(Y_sparse.toarray(), chunks=(1000, 1000))\n",
    "    \n",
    "    # Compute dot products and norms using Dask\n",
    "    dot_product = da.dot(X_dask, Y_dask.T)\n",
    "    X_norm = da.linalg.norm(X_dask, axis=1)\n",
    "    Y_norm = da.linalg.norm(Y_dask, axis=1)\n",
    "    \n",
    "    similarity = dot_product / (X_norm[:, None] * Y_norm[None, :])\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Compute cosine similarity for style and description in a scalable way\n",
    "style_similarity = compute_cosine_similarity_sparse(style_tfidf_sparse, style_tfidf_sparse)\n",
    "description_similarity = compute_cosine_similarity_sparse(desc_tfidf_sparse, desc_tfidf_sparse)\n",
    "\n",
    "# Average the similarities to get the combined similarity\n",
    "combined_similarity = (style_similarity + description_similarity) / 2\n",
    "\n",
    "print(combined_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4202aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
